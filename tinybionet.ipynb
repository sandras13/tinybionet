{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUKAi5cjT3M8",
        "outputId": "7ec1d126-666e-4b3a-fe6d-8d18e9ef91bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiRR9W9AQAu7",
        "outputId": "89b98dd6-b3a8-4b99-982a-284fab5179ec"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow_model_optimization\n",
        "!pip install keras_tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "M3Ut9OUxUNud"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Imports & setup\n",
        "# ============================================================\n",
        "\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from scipy.signal import stft\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "# Optional: path to local modules\n",
        "sys.path.append('/content/drive/MyDrive/deep_learning_quantized')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Dx-n2lPbU7b8"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Configuration\n",
        "# ============================================================\n",
        "\n",
        "params = {\n",
        "    \"seed\": 256,\n",
        "    \"batch_size\": 32,\n",
        "    \"epochs\": 30,\n",
        "    \"fine_tune_epochs\": 5,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"fine_tune_lr\": 1e-4,\n",
        "    \"step_size\": 3,\n",
        "    \"gamma\": 0.85,\n",
        "    \"window_size\": 512,\n",
        "    \"overlap\": 512 * 7 // 8,\n",
        "    \"num_bits\": 8,\n",
        "    \"num_classes\": None\n",
        "}\n",
        "\n",
        "np.random.seed(params[\"seed\"])\n",
        "tf.random.set_seed(params[\"seed\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7nac3EaCIBfI"
      },
      "outputs": [],
      "source": [
        "# Choose dataset and the biomedical signal modality to be used\n",
        "\n",
        "dataset_id = 1\n",
        "data_used = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PrWJXW9MUR8S"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Data loading\n",
        "# ===========================================================\n",
        "\n",
        "def read_data_from_csv(csv_file_name):\n",
        "      data = pd.read_csv(csv_file_name)\n",
        "      x_data = data.iloc[:, :-2].values\n",
        "      y_data = data.iloc[:, -2].values\n",
        "      subj = data.iloc[:, -1].values\n",
        "\n",
        "      return x_data, y_data, subj\n",
        "\n",
        "if dataset_id == 1:\n",
        "  csv_file_name = '/content/drive/MyDrive/deep_learning_quantized/multimodal_data.csv'\n",
        "  labels = ('low', 'medium', 'high')\n",
        "  avg = True\n",
        "  fs = 64\n",
        "  step_size = 16\n",
        "\n",
        "elif dataset_id == 2:\n",
        "  csv_file_name = '/content/drive/MyDrive/deep_learning_quantized/combined_data.csv'\n",
        "  labels = ('rest', 'squat', 'step')\n",
        "  avg = False\n",
        "  fs = 400\n",
        "  step_size = 64\n",
        "\n",
        "else:\n",
        "  csv_file_name = '/content/drive/MyDrive/deep_learning_quantized/output_dataset.csv'\n",
        "  labels = ('baseline', 'stress', 'amusement', 'meditation')\n",
        "  avg = False\n",
        "  fs = 64\n",
        "  step_size = 16\n",
        "\n",
        "features, targets, subj_data = read_data_from_csv(csv_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwUybWorUVxl",
        "outputId": "83f71587-ceec-4167-a181-97646a9ef129"
      },
      "outputs": [],
      "source": [
        "# Biomedical signal (modality) selection\n",
        "\n",
        "if data_used == 1: # PPG only\n",
        "    features = features[:, 0].reshape(-1, 1)\n",
        "\n",
        "elif data_used == 2:  # ACC only (3-axis accelerometer)\n",
        "    features = features[:, 1:4]\n",
        "\n",
        "elif data_used == 3: # ECG only\n",
        "    features = features[:, 4].reshape(-1, 1)\n",
        "\n",
        "elif data_used == 4: # EDA only\n",
        "    features = features[:, 5].reshape(-1, 1)\n",
        "\n",
        "elif data_used == 5: # EMG only\n",
        "    features = features[:, 6].reshape(-1, 1)\n",
        "\n",
        "else: # Combined PPG + ACC (multimodal baseline)\n",
        "    features = features[:, 0:4]\n",
        "\n",
        "print(\"Selected feature shape:\", features.shape)\n",
        "print(\"Targets shape:\", targets.shape)\n",
        "\n",
        "# features: (N, C) where C depends on selected modality\n",
        "# targets:  (N,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xLZ8oLemUZmm"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Sliding window segmentation\n",
        "# ============================================================\n",
        "\n",
        "def map_values(val):\n",
        "    \"\"\"\n",
        "    Map continuous affective scores to discrete classes.\n",
        "\n",
        "    This mapping is used for the AffectiveROAD dataset, where labels\n",
        "    are provided as continuous values (e.g., stress intensity).\n",
        "    Thresholds define low / medium / high affective states.\n",
        "    \"\"\"\n",
        "    if val < 0.4:\n",
        "        return 0   # low\n",
        "    if val < 0.75:\n",
        "        return 1   # medium\n",
        "    return 2       # high\n",
        "\n",
        "\n",
        "def apply_sliding_window(features, targets, subj_data,\n",
        "                         window_size, overlap, avg=False):\n",
        "    \"\"\"\n",
        "    Segment the time series into overlapping sliding windows.\n",
        "\n",
        "    Label assignment strategy:\n",
        "    ---------------------------\n",
        "    - avg = True:\n",
        "        Used for datasets with continuous labels (e.g., AffectiveROAD).\n",
        "        Labels within a window are averaged and mapped to discrete classes\n",
        "        using `map_values`.\n",
        "\n",
        "    - avg = False:\n",
        "        Used for datasets with discrete class labels.\n",
        "        The window label is determined by majority voting.\n",
        "\n",
        "    Subject consistency:\n",
        "    --------------------\n",
        "    Windows spanning multiple subjects are discarded to avoid\n",
        "    subject leakage.\n",
        "    \"\"\"\n",
        "\n",
        "    sliding_X_data = []\n",
        "    sliding_y_data = []\n",
        "    i = 0\n",
        "\n",
        "    while i < len(features) - window_size:\n",
        "        window_X = features[i:i + window_size]\n",
        "        window_y = targets[i:i + window_size]\n",
        "        subj_window = subj_data[i:i + window_size]\n",
        "\n",
        "        # Ensure all samples in the window belong to the same subject\n",
        "        if len(np.unique(subj_window)) == 1:\n",
        "\n",
        "            if avg:\n",
        "                # Continuous labels → average + thresholding\n",
        "                y_avg = np.mean(window_y)\n",
        "                y_label = map_values(y_avg)\n",
        "\n",
        "            else:\n",
        "                # Discrete labels → majority vote\n",
        "                y_label = Counter(window_y).most_common(1)[0][0]\n",
        "\n",
        "            sliding_X_data.append(window_X)\n",
        "            sliding_y_data.append(y_label)\n",
        "\n",
        "        # Move window forward\n",
        "        i += (window_size - overlap)\n",
        "\n",
        "    return (\n",
        "        np.array(sliding_X_data),\n",
        "        np.array(sliding_y_data).reshape(-1,)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oStPVpGVUZEh",
        "outputId": "74bfcb3b-f165-4138-b760-1e7f9c374bba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_data shape: (2444, 512, 1)\n",
            "y_data shape: (2444,)\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------------------------------\n",
        "# Signal downsampling and segmentation\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# To reduce temporal redundancy and computational cost,\n",
        "# every `step_size`-th sample is retained.\n",
        "features_ds = features[::step_size]\n",
        "targets_ds  = targets[::step_size]\n",
        "subj_ds     = subj_data[::step_size]\n",
        "\n",
        "# Sliding window segmentation\n",
        "X_win, y_win = apply_sliding_window(\n",
        "    features_ds,\n",
        "    targets_ds,\n",
        "    subj_ds,\n",
        "    params[\"window_size\"],\n",
        "    params[\"overlap\"],\n",
        "    avg\n",
        ")\n",
        "\n",
        "# Final tensors used for training\n",
        "X_data = X_win.astype(np.float32)\n",
        "y_data = y_win.astype(np.uint8)\n",
        "\n",
        "print(\"X_data shape:\", X_data.shape)\n",
        "print(\"y_data shape:\", y_data.shape)\n",
        "\n",
        "params[\"num_classes\"] = len(np.unique(y_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "jU_a6xZbJaAC"
      },
      "outputs": [],
      "source": [
        "def compute_stft_features(X, fs=64, nperseg=32):\n",
        "    \"\"\"Compute real + imaginary STFT per channel.\"\"\"\n",
        "    stft_out = []\n",
        "\n",
        "    for sample in X:\n",
        "        channels = []\n",
        "        for ch in range(sample.shape[1]):\n",
        "            _, _, Zxx = stft(sample[:, ch], fs=fs, nperseg=nperseg)\n",
        "            channels.append(np.real(Zxx))\n",
        "            channels.append(np.imag(Zxx))\n",
        "        stft_out.append(np.stack(channels, axis=-1))\n",
        "\n",
        "    stft_out = np.array(stft_out)\n",
        "    stft_out = stft_out[:, 1:, 1:]  # remove DC bins\n",
        "\n",
        "    # Per-sample, per-channel normalization\n",
        "    for i in range(len(stft_out)):\n",
        "        mean = stft_out[i].mean(axis=(0, 1), keepdims=True)\n",
        "        std = stft_out[i].std(axis=(0, 1), keepdims=True)\n",
        "        stft_out[i] = (stft_out[i] - mean) / (std + 1e-8)\n",
        "\n",
        "    return stft_out.astype(np.float32)\n",
        "\n",
        "\n",
        "X_stft = compute_stft_features(X_win, fs // step_size, 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "dpoGbu_-EBnZ"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Uniform int8 quantization (input-side)\n",
        "# ============================================================\n",
        "\n",
        "def quantize_array(x, num_bits=8):\n",
        "    \"\"\"Symmetric uniform quantization.\"\"\"\n",
        "    qmax = (2 ** (num_bits - 1)) - 1\n",
        "    scale = np.max(np.abs(x)) / qmax if np.max(np.abs(x)) != 0 else 1.0\n",
        "    x_q = np.clip(np.round(x / scale), -qmax - 1, qmax)\n",
        "    return x_q.astype(np.int8)\n",
        "\n",
        "\n",
        "X_data = quantize_array(X_stft, params[\"num_bits\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "pjbBNMqBOL7n"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Model definition\n",
        "# ============================================================\n",
        "\n",
        "def create_functional_resnet(input_shape, num_classes):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    x = layers.Conv2D(8, 3, strides=2, padding=\"same\", activation=\"relu\")(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv2D(16, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    shortcut = x\n",
        "\n",
        "    x = layers.Conv2D(32, 1, activation=\"relu\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.DepthwiseConv2D(3, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv2D(16, 1, activation=\"relu\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Add()([shortcut, x])\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    return models.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "CD7jKGHCJqHV"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Weight quantization (post-training)\n",
        "# ============================================================\n",
        "\n",
        "def quantize_tensor(x, num_bits):\n",
        "    qmax = (2 ** (num_bits - 1)) - 1\n",
        "    scale = tf.reduce_max(tf.abs(x)) / qmax\n",
        "    scale = tf.where(scale == 0, 1.0, scale)\n",
        "    return tf.clip_by_value(tf.round(x / scale), -qmax - 1, qmax) * scale\n",
        "\n",
        "\n",
        "def apply_weight_quantization(model, num_bits):\n",
        "    \"\"\"Quantize Conv2D and Dense weights.\"\"\"\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, (layers.Conv2D, layers.Dense, layers.DepthwiseConv2D)):\n",
        "            weights = layer.get_weights()\n",
        "            if weights:\n",
        "                q_weights = [quantize_tensor(tf.constant(w), num_bits).numpy() for w in weights]\n",
        "                layer.set_weights(q_weights)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "AGftyztWLumc"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Learning rate scheduler\n",
        "# ============================================================\n",
        "\n",
        "def lr_schedule(epoch, lr):\n",
        "    if epoch > 0 and epoch % params[\"step_size\"] == 0:\n",
        "        return lr * params[\"gamma\"]\n",
        "    return lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "k1sIl-mAKn9R"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# TFLite quantization\n",
        "# ============================================================\n",
        "\n",
        "def representative_data_gen(X):\n",
        "    \"\"\"\n",
        "    Representative dataset for full integer quantization.\n",
        "    Uses a small subset of training data.\n",
        "    \"\"\"\n",
        "    for i in range(min(100, len(X))):\n",
        "        yield [X[i:i+1].astype(np.float32)]\n",
        "\n",
        "def evaluate_tflite_model(tflite_model, X, y_true):\n",
        "    \"\"\"Run inference with a TFLite int8 model.\"\"\"\n",
        "    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    y_pred = []\n",
        "\n",
        "    for i in range(len(X)):\n",
        "        interpreter.set_tensor(\n",
        "            input_details[0][\"index\"],\n",
        "            X[i:i+1].astype(np.float32)\n",
        "        )\n",
        "        interpreter.invoke()\n",
        "        output = interpreter.get_tensor(output_details[0][\"index\"])\n",
        "        y_pred.append(output)\n",
        "\n",
        "    y_pred = np.argmax(np.vstack(y_pred), axis=1)\n",
        "    acc = np.mean(y_pred == y_true)\n",
        "    f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "\n",
        "    return acc, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yNhJ1Ba-J6JR",
        "outputId": "11b434d9-e8de-423f-f76f-15a209869351"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "num_folds = 5\n",
        "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=params[\"seed\"])\n",
        "\n",
        "acc_scores, f1_scores = [], []\n",
        "tflite_acc_scores, tflite_f1_scores = [], []\n",
        "\n",
        "def evaluate_tflite_model(tflite_model, X, y_true):\n",
        "    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "    interpreter.allocate_tensors()\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    # Quantize input to int8\n",
        "    scale, zero_point = input_details[0]['quantization']\n",
        "    X_int8 = (X / scale + zero_point).astype(np.int8)\n",
        "\n",
        "    y_pred_list = []\n",
        "    for i in range(len(X_int8)):\n",
        "        interpreter.set_tensor(input_details[0]['index'], X_int8[i:i+1])\n",
        "        interpreter.invoke()\n",
        "        out = interpreter.get_tensor(output_details[0]['index'])\n",
        "        # Dequantize output\n",
        "        out_float = (out.astype(np.float32) - output_details[0]['quantization'][1]) * output_details[0]['quantization'][0]\n",
        "        y_pred_list.append(out_float)\n",
        "\n",
        "    y_pred = np.vstack(y_pred_list)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    acc = np.mean(y_pred_classes == y_true)\n",
        "    f1 = f1_score(y_true, y_pred_classes, average=\"weighted\")\n",
        "    return acc, f1\n",
        "\n",
        "for fold, (tr_idx, val_idx) in enumerate(skf.split(X_data, y_win), 1):\n",
        "    print(f\"\\n{'='*40}\\nFold {fold}\\n{'='*40}\")\n",
        "\n",
        "    # Split data\n",
        "    X_tr, X_val = X_data[tr_idx], X_data[val_idx]\n",
        "    y_tr, y_val = y_win[tr_idx], y_win[val_idx]\n",
        "\n",
        "    # One-hot encoding for Keras\n",
        "    y_tr_cat = tf.keras.utils.to_categorical(y_tr, params[\"num_classes\"])\n",
        "    y_val_cat = tf.keras.utils.to_categorical(y_val, params[\"num_classes\"])\n",
        "\n",
        "    # --- Step 1: Create float32 model ---\n",
        "    model = create_functional_resnet(X_tr.shape[1:], params[\"num_classes\"])\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.AdamW(params[\"learning_rate\"]),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    if fold == 1: print(model.summary())\n",
        "\n",
        "    # Train float model\n",
        "    model.fit(\n",
        "        X_tr, y_tr_cat,\n",
        "        epochs=params[\"epochs\"],\n",
        "        batch_size=params[\"batch_size\"],\n",
        "        validation_data=(X_val, y_val_cat),\n",
        "        callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule)],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Apply weight quantization ---\n",
        "    model_quant = apply_weight_quantization(model, params[\"num_bits\"])\n",
        "\n",
        "    # --- Step 3: Fine-tune quantized model ---\n",
        "    model_quant.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(params[\"fine_tune_lr\"]),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    model_quant.fit(\n",
        "        X_tr, y_tr_cat,\n",
        "        epochs=params[\"fine_tune_epochs\"],\n",
        "        batch_size=8,\n",
        "        validation_data=(X_val, y_val_cat),\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # --- Step 4: Evaluate Keras model ---\n",
        "    val_loss, val_acc = model_quant.evaluate(X_val, y_val_cat, verbose=0)\n",
        "    y_pred_classes = np.argmax(model_quant.predict(X_val), axis=1)\n",
        "    f1 = f1_score(y_val, y_pred_classes, average=\"weighted\")\n",
        "\n",
        "    acc_scores.append(val_acc)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    print(\"Keras float/quantized model metrics:\")\n",
        "    print(classification_report(y_val, y_pred_classes))\n",
        "\n",
        "    # --- Step 5: Convert to INT8 TFLite ---\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model_quant)\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "    # Use representative dataset for full integer quantization\n",
        "    def representative_data_gen():\n",
        "        for i in range(100):\n",
        "            yield [X_tr[i:i+1].astype(np.float32)]\n",
        "    converter.representative_dataset = representative_data_gen\n",
        "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "    converter.inference_input_type = tf.int8\n",
        "    converter.inference_output_type = tf.int8\n",
        "\n",
        "    tflite_model = converter.convert()\n",
        "\n",
        "    # --- Step 6: Evaluate TFLite INT8 model ---\n",
        "    tflite_acc, tflite_f1 = evaluate_tflite_model(tflite_model, X_val, y_val)\n",
        "    tflite_acc_scores.append(tflite_acc)\n",
        "    tflite_f1_scores.append(tflite_f1)\n",
        "\n",
        "    print(f\"TFLite INT8 model - Accuracy: {tflite_acc:.4f}, F1: {tflite_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cBlRZ48J8VU"
      },
      "outputs": [],
      "source": [
        "# --- Summary ---\n",
        "print(\"\\n===== Cross-validation results =====\")\n",
        "print(\"Keras Accuracy:\", np.mean(acc_scores), \"F1:\", np.mean(f1_scores))\n",
        "print(\"TFLite INT8 Accuracy:\", np.mean(tflite_acc_scores), \"F1:\", np.mean(tflite_f1_scores))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
